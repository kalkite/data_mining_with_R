---
title: "Week-2"
author: "Rajesh Kalakoti"
date: "2023-08-03"
output:
  pdf_document:
    latex_engine: xelatex
    keep_tex: yes
    number_sections: true
  html_document:
    toc: true
    toc_depth: 3
header-includes:
  - \usepackage{algpseudocode}
  - \usepackage[linesnumbered,ruled,lined,boxed]{algorithm2e}
  - \usepackage{amsmath}
  - \usepackage{amsfonts}
  - \usepackage{placeins}
  - \usepackage{float}
  - \usepackage{xcolor}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(reticulate)
library(here)
```

* Packages
    + [devtools](https://www.r-project.org/nosvn/pandoc/devtools.html) 
    + [tidyverse](https://www.tidyverse.org/packages/)
    + here
    + devtools::install_github("jeffwong/pdist")
    + donut
    + rnn

```{r}
library(here)
project_path <- here()
source(here("R","utils.R"))
source(here("R","distance_functions.R"))
```


# Clustering

Given a clustering \(C = \{C_1, C_2, \ldots, C_k\}\), we need some scoring function that evaluates its quality or goodness. This sum of squared errors scoring function is defined as:
\[ W(C) = \frac{1}{2} \sum_{k=1}^{K} \sum_{i: C(i)=k} \|x_i - \bar{x}_k\|^2 \]
The goal is to find the clustering that minimizes:
\[ C^* = \arg \min_C \{ W(c) \} \]

## Classification of clustering techniques
Most common clustering techniques may be classified as follows:

- **Representative Based Techniques**
    - k-means, k-medians, k-medoids, etc. Each cluster has a representative which is either the
    element of the data set or an element from the same space as all other elements of the dataset.
    Shape of the clusters is affected by the choice of
    distance function. Number of clusters is usually a hyperparameter.

- **Hierarchical Clustering Techniques**
    - Agglomerative and Divisive techniques. Not always relies on the distance function. Different levels of clustering granularity provide different provide different application specific insides.

- **Grid and Density Based Techniques**
    - Relies on the local density of the data points. Well suited for the clusters of irregular shapes.

- **Probabilistic Algorithms**
    - Examples: EM (Expectation-Maximization) and EM-like algorithms.  Utilize probabilistic models for clustering



## Hopkins Statistic
The Hopkins statistic (Lawson and Jurs 1990) is used to assess the clustering tendency of a data set by measuring the probability that a given data set is generated by a uniform data distribution

Let \( \mathcal{D} \) be the data set to investigate and \( \mathcal{R} \) is a representative sample of \( \mathcal{D} \), of power \( \mathcal{r} \). \( \mathcal{S} \) is a synthetic data set of \( \mathcal{r} \) data points randomly generated from the same domain. Let \( \mathcal{\alpha_1} \), . . . \( \mathcal{\alpha_r} \) be the distances of each point of R to the nearest neighbour in \( \mathcal{D} \) and \( \mathcal{\beta_1} \), . . . \( \mathcal{\beta_r} \) are the distances of each point of \( \mathcal{S} \) to the nearest neighbour in \( \mathcal{D} \). The Hopkins statistic is defined as follows:

\[ H = \frac{\sum_{i=1}^{r} \beta_i}{\sum_{i=1}^{r}(\alpha_i+\beta_i)} \]


- **Hopkins Statistic**
  - Used to assess clustering tendency of a dataset. Measures likelihood of dataset having a cluster structure based on distances between data points.
  - Ranges between  0 and 1 .

- **Interpretation of Values**
  - Higher values of H indicate highly clustered data.
  
\textcolor{red}{Example}
```{r}
# Load the Iris dataset
data(iris)

# Calculate the Hopkins statistic for the data columns. 
hopkins_result <- hopkins_stat(iris[, 1:4])

# Print the calculated Hopkins statistic with a description
cat("Hopkins Statistic for the Iris dataset:", hopkins_result, "\n")
```

## Representative Based Techniques algorithms 

### K-means

K-means employs a greedy iterative approach to find a clustering that minimizes loss function. 
\begin{algorithm}
\LinesNumbered % Add line numbers to the algorithm
\caption{K-means Algorithm}
\KwData{$D, k, \varepsilon$}

\textbf{K-means}($D, k, \varepsilon$): {
  
  $t \leftarrow 0$\;
  Randomly initialize $k$ centroids: $\mu_{1}^{t}, \mu_{2}^{t}, \ldots, \mu_{n}^{t} \in \mathbb{R}^d$\;
  \Repeat{$\sum_{i=1}^{k} ||\mu_{i}^t - \mu_{i}^{t-1}||^2 \le \varepsilon $}{
    $t \leftarrow t+1$\;
    $C_i \leftarrow \emptyset$ for all \(i = 1, \ldots, k\)
    
    /* Cluster assignment step */

    
    \For{$x_j \in D$}{
      $i^* \leftarrow \text{argmin}_i \{||x_j - \mu_{i}^{t-1}||^2 \}$\;
      /* assign $x_j$ to closest centroid */
      
      $C_{i^*} \leftarrow C_{i^*} \cup \{x_j\}$\;
    }
    
        
    \For{$i = 1, .., k $}{
      $  \mu_{i}^{t} \leftarrow \frac{1}{|C_i|} \sum_{x_j \in C_i} X_j $
    }
  }
}
\end{algorithm}


```{r}

```

