---
title: "Week-2"
author: "Rajesh Kalakoti"
date: "2023-08-03"
output:
  pdf_document:
    latex_engine: xelatex
    keep_tex: yes
    number_sections: true
  html_document:
    toc: true
    toc_depth: 3
header-includes:
  - \usepackage{algpseudocode}
  - \usepackage[linesnumbered,ruled,lined,boxed]{algorithm2e}
  - \usepackage{amsmath}
  - \usepackage{amsfonts}
  - \usepackage{placeins}
  - \usepackage{float}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(reticulate)
library(here)
```

* Packages
    + [devtools](https://www.r-project.org/nosvn/pandoc/devtools.html) 
    + [tidyverse](https://www.tidyverse.org/packages/)
    + here

```{r}
library(here)
project_path <- here()
source(here("R","utils.R"))
source(here("R","distance_functions.R"))
```


# Clustering

Given a clustering \(C = \{C_1, C_2, \ldots, C_k\}\), we need some scoring function that evaluates its quality or goodness. This sum of squared errors scoring function is defined as:
\[ W(C) = \frac{1}{2} \sum_{k=1}^{K} \sum_{i: C(i)=k} \|x_i - \bar{x}_k\|^2 \]
The goal is to find the clustering that minimizes:
\[ C^* = \arg \min_C \{ W(c) \} \]

## Classification of clustering techniques
Most common clustering techniques may be classified as follows:

- **Representative Based Techniques**
    - k-means, k-medians, k-medoids, etc. Each cluster has a representative which is either the
    element of the data set or an element from the same space as all other elements of the dataset.
    Shape of the clusters is affected by the choice of
    distance function. Number of clusters is usually a hyperparameter.

- **Hierarchical Clustering Techniques**
    - Agglomerative and Divisive techniques. Not always relies on the distance function. Different levels of clustering granularity provide different provide different application specific insides.

- **Grid and Density Based Techniques**
    - Relies on the local density of the data points. Well suited for the clusters of irregular shapes.

- **Probabilistic Algorithms**
    - Examples: EM (Expectation-Maximization) and EM-like algorithms.  Utilize probabilistic models for clustering

K-means employs a greedy iterative approach to find a clustering that minimizes loss function. 
\begin{algorithm}
\LinesNumbered % Add line numbers to the algorithm
\caption{K-means Algorithm}
\KwData{$D, k, \varepsilon$}

\textbf{K-means}($D, k, \varepsilon$): {
  
  $t \leftarrow 0$\;
  Randomly initialize $k$ centroids: $\mu_{1}^{t}, \mu_{2}^{t}, \ldots, \mu_{n}^{t} \in \mathbb{R}^d$\;
  \Repeat{$\sum_{i=1}^{k} ||\mu_{i}^t - \mu_{i}^{t-1}||^2 \le \varepsilon $}{
    $t \leftarrow t+1$\;
    $C_i \leftarrow \emptyset$ for all \(i = 1, \ldots, k\)
    
    /* Cluster assignment step */

    
    \For{$x_j \in D$}{
      $i^* \leftarrow \text{argmin}_i \{||x_j - \mu_{i}^{t-1}||^2 \}$\;
      /* assign $x_j$ to closest centroid */
      
      $C_{i^*} \leftarrow C_{i^*} \cup \{x_j\}$\;
    }
    
        
    \For{$i = 1, .., k $}{
      $  \mu_{i}^{t} \leftarrow \frac{1}{|C_i|} \sum_{x_j \in C_i} X_j $
    }
  }
}
\end{algorithm}



```{r}



```

