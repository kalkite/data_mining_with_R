---
title: "Week-2"
author: "Rajesh Kalakoti"
date: "2023-08-03"
output:
  pdf_document:
    latex_engine: xelatex
    keep_tex: yes
    number_sections: true
  html_document:
    toc: true
    toc_depth: 3
header-includes:
  - \usepackage{algpseudocode}
  - \usepackage[linesnumbered,ruled,lined,boxed]{algorithm2e}
  - \usepackage{amsmath}
  - \usepackage{amsfonts}
  - \usepackage{placeins}
  - \usepackage{float}
  - \usepackage{xcolor}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(reticulate)
library(here)
```

* Packages
    + [devtools](https://www.r-project.org/nosvn/pandoc/devtools.html) 
    + [tidyverse](https://www.tidyverse.org/packages/)
    + here
    + devtools::install_github("jeffwong/pdist")
    + donut
    + rnn

```{r}
library(here)
project_path <- here()
source(here("R","utils.R"))
source(here("R","distance_functions.R"))
fraud_data_file = here("data","PS_20174392719_1491204439457_log.csv")
```


# Clustering

Given a clustering \(C = \{C_1, C_2, \ldots, C_k\}\), we need some scoring function that evaluates its quality or goodness. This sum of squared errors scoring function is defined as:
\[ W(C) = \frac{1}{2} \sum_{k=1}^{K} \sum_{i: C(i)=k} \|x_i - \bar{x}_k\|^2 \]
The goal is to find the clustering that minimizes:
\[ C^* = \arg \min_C \{ W(c) \} \]

## Classification of clustering techniques
Most common clustering techniques may be classified as follows:

- **Representative Based Techniques**
    - k-means, k-medians, k-medoids, etc. Each cluster has a representative which is either the
    element of the data set or an element from the same space as all other elements of the dataset.
    Shape of the clusters is affected by the choice of
    distance function. Number of clusters is usually a hyperparameter.

- **Hierarchical Clustering Techniques**
    - Agglomerative and Divisive techniques. Not always relies on the distance function. Different levels of clustering granularity provide different provide different application specific insides.

- **Grid and Density Based Techniques**
    - Relies on the local density of the data points. Well suited for the clusters of irregular shapes.

- **Probabilistic Algorithms**
    - Examples: EM (Expectation-Maximization) and EM-like algorithms.  Utilize probabilistic models for clustering



## Hopkins Statistic
The Hopkins statistic (Lawson and Jurs 1990) is used to assess the clustering tendency of a data set by measuring the probability that a given data set is generated by a uniform data distribution

Let \( \mathcal{D} \) be the data set to investigate and \( \mathcal{R} \) is a representative sample of \( \mathcal{D} \), of power \( \mathcal{r} \). \( \mathcal{S} \) is a synthetic data set of \( \mathcal{r} \) data points randomly generated from the same domain. Let \( \mathcal{\alpha_1} \), . . . \( \mathcal{\alpha_r} \) be the distances of each point of R to the nearest neighbour in \( \mathcal{D} \) and \( \mathcal{\beta_1} \), . . . \( \mathcal{\beta_r} \) are the distances of each point of \( \mathcal{S} \) to the nearest neighbour in \( \mathcal{D} \). The Hopkins statistic is defined as follows:

\[ H = \frac{\sum_{i=1}^{r} \beta_i}{\sum_{i=1}^{r}(\alpha_i+\beta_i)} \]


- **Hopkins Statistic**
  - Used to assess clustering tendency of a dataset. Measures likelihood of dataset having a cluster structure based on distances between data points.
  - Ranges between  0 and 1 .

- **Interpretation of Values**
  - Higher values of H indicate highly clustered data.
  
\textcolor{red}{Example}
```{r}
# Load the Iris dataset
data(iris)

# Calculate the Hopkins statistic for the data columns. 
hopkins_result <- hopkins_stat(iris[, 1:4])

# Print the calculated Hopkins statistic with a description
cat("Hopkins Statistic for the Iris dataset:", hopkins_result, "\n")
```

## Representative Based Techniques algorithms 

### K-means

K-means employs a greedy iterative approach to find a clustering that minimizes loss function. 
\begin{algorithm}
\LinesNumbered % Add line numbers to the algorithm
\caption{K-means Algorithm}
\KwData{$D, k, \varepsilon$}

\textbf{K-means}($D, k, \varepsilon$): {
  
  $t \leftarrow 0$\;
  Randomly initialize $k$ centroids: $\mu_{1}^{t}, \mu_{2}^{t}, \ldots, \mu_{n}^{t} \in \mathbb{R}^d$\;
  \Repeat{$\sum_{i=1}^{k} ||\mu_{i}^t - \mu_{i}^{t-1}||^2 \le \varepsilon $}{
    $t \leftarrow t+1$\;
    $C_i \leftarrow \emptyset$ for all \(i = 1, \ldots, k\)
    
    /* Cluster assignment step */

    
    \For{$x_j \in D$}{
      $i^* \leftarrow \text{argmin}_i \{||x_j - \mu_{i}^{t-1}||^2 \}$\;
      /* assign $x_j$ to closest centroid */
      
      $C_{i^*} \leftarrow C_{i^*} \cup \{x_j\}$\;
    }
    
        
    \For{$i = 1, .., k $}{
      $  \mu_{i}^{t} \leftarrow \frac{1}{|C_i|} \sum_{x_j \in C_i} X_j $
    }
  }
}
\end{algorithm}

# Dataset

This dataset is a synthetic dataset generated using the simulator called PaySim. The dataset contains financial transactions with fraud observations. For more details, you can check [here](https://www.kaggle.com/datasets/ealaxi/paysim1).

```{r}
fraud = read.csv(fraud_data_file)
str(fraud)
```

```{r}
library(readr)
library(tidyverse)
library(ggplot2)
dims <- dim(fraud)
print(paste("dimensions:", paste(dims, collapse = "x")))
```
The data consist of more than 2263777 million observations and 11 variables. Since the data is large, cleaning the dataset beforehand is a prerequisite for efficiency as the computer will have to deal with a large amount of data.

The first thing we need to remove the variables that do not contribute to the investigation. The variable of nameOrig and nameDest consist of a vast amount of unique values. We create a new object to differentiate from the uncleaned dataset and drop these two variables as our first step.

```{r}
fraud_clean <- fraud %>%  select(-c(nameOrig, nameDest))
head(fraud_clean,10)
```

the second step is we convert the variable that is not suitable for the analysis. The variables of type are under character type. This variable is important for the identification of the type of transaction. We will convert this one into a factor type.


```{r}
fraud_clean <- fraud_clean %>% 
  mutate(type = as.factor(type),
         isFraud = as.factor(isFraud))
```


## Data Exploration

```{r}
transaction <- c("fraud","genuine")
value <- c(sum(fraud_clean$isFraud == 1), sum(fraud_clean$isFraud == 0))
percentage <- c(sum(fraud_clean$isFraud == 1)/length(fraud_clean$isFraud)*100,
                sum(fraud_clean$isFraud == 0)/length(fraud_clean$isFraud)*100)
information <- data.frame(transaction,value,percentage)
information
```
The fraud transaction is tiny compared to the genuine transaction. It only contains less than 1% of the overall data. We try to visualize the whole transactions based on the transaction type.

```{r}
total_fraud <- fraud_clean %>% 
  filter(isFraud == 1) %>% 
  select(type) %>% 
  group_by(type) %>% 
  count(type) %>% 
  arrange(desc(n))
total_fraud
```
```{r}
fraud_clean_real <- fraud_clean %>% 
  filter(type == "CASH_OUT" | type == "TRANSFER")
```

```{r}
RNGkind(sample.kind = "Rounding")
set.seed(11)
library(rsample)
fraud_split <- initial_split(data = fraud_clean_real, prop = 0.7, strata = isFraud)
fraud_train <- training(fraud_split)
fraud_test <- testing(fraud_split)
```

```{r}
fraud_scale_train <- fraud_train %>% select(-c(isFraud, isFlaggedFraud, type))
fraud_scale_train <- as.data.frame(scale(fraud_scale_train))

fraud_scale_test <- fraud_test %>% select(-c(isFraud, isFlaggedFraud, type))
fraud_scale_test <- as.data.frame(scale(fraud_scale_test))
```



```{r}
# Create a data frame with the provided data
```



```{r}
library(dbscan)
```


```{r}
test_samples = head(fraud_scale_test,100)
fraud_lof1 <- lof(test_samples, k = 3)
```


```{r}
source(here("R", "loc_out_fact.R"))
```

```{r}
lof_2 = local_outlier_factor(test_samples,3)
```




