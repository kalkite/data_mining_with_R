---
title: "Week-4"
author: "Rajesh Kalakoti, Sven nomm"
date: "2023-08-03"
output:
  html_document:
    toc: true
    # toc_depth: 3
  pdf_document:
    latex_engine: xelatex
    keep_tex: yes
    number_sections: true
header-includes:
  - \usepackage{algpseudocode}
  - \usepackage[linesnumbered,ruled,lined,boxed]{algorithm2e}
  - \usepackage{amsmath}
  - \usepackage{amsfonts}
  - \usepackage{placeins}
  - \usepackage{float}
  - \usepackage{xcolor}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Load the conflicted package
library(conflicted)
library(tidyverse)
library(reticulate)
library(here)
# Specify which function to use explicitly
conflict_prefer("filter", "dplyr")
conflict_prefer("lag", "dplyr")
fraud_data_file = here("data","PS_20174392719_1491204439457_log.csv")
```

# Local Outlier Factors

LOF is a normalized density based approach Denote $V^k(X)$ - distance to it's $k$- -nearest neighbour and $L_k(X)$ the set of points within the set of points within $k$-nearest
neighbor distance of $X$


*   Reachability distance of X with respect to Y is defined as:

$$
R_k\left( X,Y\right) = max \left\{ S(X,Y), V^k(Y) \right\}
$$

*   The average reachability distance:
$$
AR_k(X) = \frac{\sum_{y \in L_k(X)} R_k(X,Y)}{|Y \in L_k(X)|}
$$

*   Local Outlier Factor:
$$
LOF_k(X) = \frac{\sum_{y \in L_k(X)} \frac{AR_k(X)}{AR_k(Y)} }{|Y \in L_k(X)|}
$$

## K-nearest neighbor vector


```{r}
k_nneigh_vect <- function(x,data,k) {
  temp <- as.matrix(data)
  numrow <- dim(data)[1L]
  dimnames(temp) <- NULL
  # subtract rowvector x from each row of data
  difference<- scale(temp, x, FALSE)

  # square and add all differences and then take the square root
  dtemp <- drop(difference^2 %*% rep(1, ncol(data)))
  dtemp <- sqrt(dtemp)

  # order the distances
  order_dist <- order(dtemp)
  nndist <- dtemp[order_dist]

  # find distance to k-nearest neighbor
  # uses k+1 since first distance in vector is a 0
  knndist <- nndist[k+1]

  # find neighborhood
  # eliminate first row of zeros from neighborhood
  neighborhood <- drop(nndist[nndist<=knndist])
  neighborhood <- neighborhood[-1]
  numneigh <- length(neighborhood)

  # find indexes of each neighbor in the neighborhood
  index.neigh <- order_dist[1:numneigh+1]

  # this will become the index of the distance to first neighbor
  num1 <- numneigh+3

  # this will become the index of the distance to last neighbor
  num2 <- numneigh+numneigh+2

  return(c(num1,num2,index.neigh,neighborhood))
}
```

## k-distance neighborhood

```{r}
dist_to_knn <- function(dataset,neighbors) {

  numrow <- dim(dataset)[1L]
  mxNN <- neighbors*2+2
  knndist <- matrix(0,nrow=mxNN,ncol=numrow)
  for (i in 1:numrow) {
    # find obervations that make up the k-distance neighborhood for observation dataset[i,]
    neighdist <- k_nneigh_vect(dataset[i,],dataset,neighbors)

    x <- length(neighdist)
    if (x > mxNN) {
      knndist <- rbind(knndist,matrix(rep(0,(x-mxNN)*numrow),ncol=numrow))
      mxNN <- x
    }
    knndist[1:x,i] <- neighdist
  }

  return(knndist[1:mxNN,])
}
```

## Reachability Density

```{r}
reachability_density<- function(distdata,k) {

  p <- dim(distdata)[2]
  lrd <- rep(0,p)

  for (i in 1:p) {
    j <- seq(3,3+(distdata[2,i]-distdata[1,i]))
    # compare the k-distance from each observation to its kth neighbor
    # to the actual distance between each observation and its neighbors
    numneigh <- distdata[2,i]-distdata[1,i]+1
    temp <- rbind(diag(distdata[distdata[2,distdata[j,i]],distdata[j,i]]),distdata[j+numneigh,i])

    #calculate reachability
    reach <- 1/(sum(apply(temp,2,max))/numneigh)
    lrd[i] <- reach
  }
  lrd
}
```


## Calculate Local outlier Factor for each observation

```{r}
local_outlier_factor <- function(data,k) {

  data <- as.matrix(data)

  # obtain the k nearest neighbors and their distance from each observation
  distdata <- dist_to_knn(data,k)
  p <- dim(distdata)[2L]

  # calculate the local reachability_densitydensity for each observation in data
  lrddata <- reachability_density(distdata,k)

  lof <- rep(0,p)

  # computer the local outlier factor of each observation in data
  for (i in 1:p) {
    nneigh <- distdata[2,i]-distdata[1,i]+1
    j <- seq(0,(nneigh-1))
    local.factor <- sum(lrddata[distdata[3+j,i]]/lrddata[i])/nneigh
    lof[i] <- local.factor
  }

  # return lof, a vector with the local outlier factor of each observation
  lof
}

```

# Local Outlier Factor for Anomaly Detection in Fraud Detection

This dataset is a synthetic dataset generated using the simulator called PaySim. The dataset contains financial transactions with fraud observations. For more details, you can check [here](https://www.kaggle.com/datasets/ealaxi/paysim1).

## Read Data File.

```{r}
fraud = read.csv(fraud_data_file)
str(fraud)
```
```{r}
# top 10 rows of data frame
head(fraud,10)
```

The data consist of more than 2263777 million observations and 11 variables. Since the data is large, cleaning the dataset beforehand is a prerequisite for efficiency as the computer will have to deal with a large amount of data.

The first thing we need to remove the variables that do not contribute to the investigation. The variable of nameOrig and nameDest consist of a vast amount of unique values. We create a new object to differentiate from the uncleaned dataset and drop these two variables as our first step.

```{r}
fraud_clean <- fraud %>%  select(-c(nameOrig, nameDest))
head(fraud_clean,10)
```

```{r}
fraud_clean <- fraud_clean %>%
  mutate(type = as.factor(type),
         isFraud = as.factor(isFraud))
```

## Data Exploration

```{r}
transaction <- c("fraud","genuine")
value <- c(sum(fraud_clean$isFraud == 1), sum(fraud_clean$isFraud == 0))
percentage <- c(sum(fraud_clean$isFraud == 1)/length(fraud_clean$isFraud)*100,
                sum(fraud_clean$isFraud == 0)/length(fraud_clean$isFraud)*100)
information <- data.frame(transaction,value,percentage)
information
```

The fraud transaction is tiny compared to the genuine transaction. It only contains less than 1% of the overall data. We try to visualize the whole transactions based on the transaction type.

```{r}
total_fraud <- fraud_clean %>%
  filter(isFraud == 1) %>%
  select(type) %>%
  group_by(type) %>%
  count(type) %>%
  arrange(desc(n))
```


```{r}
fraud_clean_real <- fraud_clean %>%
  filter(type == "CASH_OUT" | type == "TRANSFER")
dim(fraud_clean_real)
```


```{r}
RNGkind(sample.kind = "Rounding")
set.seed(11)
library(rsample)
fraud_split <- initial_split(data = fraud_clean_real, prop = 0.7, strata = isFraud)
fraud_train <- training(fraud_split)
fraud_test <- testing(fraud_split)
```

```{r}
fraud_scale <- fraud_clean_real %>% select(-c(isFraud, isFlaggedFraud, type))
fraud_scale <- as.data.frame(scale(fraud_scale))
```

## compute Local outlier factor values for 1000

```{r}
fraud_clean_real_1000 <- head(fraud_clean_real,1000)
fraud_scale_1000 <- head(fraud_scale,1000)
```

lof values for 1000 data points. 

```{r}
local_factor_values = local_outlier_factor(fraud_scale_1000,3)
```
appendind lof values to original data frame. 
```{r}
fraud_clean_real_1000$lof <- local_factor_values
```

To gain more understanding of the data, we will see the distribution between fraud and genuine transactions. Since the dataset consists of more than two variables, we can use PCA and use the first two dimensions of the PCA.

```{r}
library(FactoMineR)
fraud_pca <- PCA(fraud_scale_1000, scale.unit = F, ncp = 6, graph = F)
summary(fraud_pca)
```
## visualize variance of PCA 

```{r}
library("factoextra")
fviz_eig(fraud_pca, ncp = 6, addlabels = T, main = "Variance explained by each dimensions")
```

The result from PCA above shows that if we use the first two dimensions of the data, we still retain 98% variance from the original data. The first three dimensions along with the LOF score and fraud label is obtained and stored in the new data frame.

```{r}

fraud_a <- data.frame(fraud_pca$ind$coord[,1:3])
fraud_b <- cbind(fraud_a, fraud = fraud_clean_real_1000$isFraud, lof_score = fraud_clean_real_1000$lof)
fraud_lof_visual <- ggplot(fraud_b, aes(x=Dim.1 ,y=Dim.2, color=fraud)) + 
    geom_point(aes(size=lof_score)) +
  ggtitle("LOF Score Distribution")

fraud_lof_visual
```
From the visualization above, we can see that genuine and fraudulent transactions have different patterns. The higher the lof score it has, the dot is bolder and more prominent.

The rule of thumb of the lof score says that if the LOF score is more than 1, it is likely to be an outlier. Somehow, a threshold can be adjusted with the distribution of the data. Letâ€™s see the statistics of the LOF score first.

```{r}
summary(fraud_b)
```

```{r}
fraud_b %>%
  filter(lof_score <= 1.75) %>% 
  ggplot( aes(x=lof_score)) +
    geom_density( color="#e9ecef", fill = "#c90076", alpha=0.7) +
    scale_fill_manual(values="#8fce00") +
    xlab("LOF Score")+
  ggtitle("LOF Score Distribution")
    labs(fill="")
```


We see above, the LOF score have many points with the score more than 1. To classify a point as an outlier or not, we can set the threshold higher.

One method to determine threshold is calculating the quantile point. Here, we will set threshold 90% as the normal points, while the last 10% is considered as outlier. The threshold proportion can be adjusted depend on the business case. If user wish to more cautios with the LOF score, user can set the threshold higher.

```{r}
quantile(fraud_b$lof_score, probs = c(0, 0.9))
```

The 90% proportion of the LOF score falls under below 1.4561876. We will use this threshold to determine if a point falls under the threshold; we categorize that point as an outlier.


```{r}
fraud_b <- fraud_b %>% 
  mutate(outlier = ifelse(lof_score > 1.4561876 , 1, 0))
```
We can once again visualize the distribution of the outlier for all observations.

```{r}
fraud_lof_visual_b <- ggplot(fraud_b, aes(x=Dim.1 ,y=Dim.2, color=outlier)) +
    geom_point() +
  ggtitle("LOF Score Distribution")

fraud_lof_visual_b
```


# ISOLATION forest for Fraud detection. 

The algorithm of Isolation Forest works the same by targeting the observation that stands alone after the isolation is applied to the observation. The observation far from the rest will be identified as unusual rather than gathered in groups that share the similarity. The more points travel from the groups, the more indication we can consider that as an anomaly.
The isolation forest will choose attributes originating from the data in random and recursively partition the observation of the maximum and minimum value of the selected features. The observation that requires less partition will likely be an anomaly than the observation that requires more separation.


```{r}
library(isotree)
fraud_isotree <- isolation.forest(fraud_train %>% select(-isFraud), sample_size = 64)
```

The code will build the algorithm with the default parameter. The default parameters are:

- **sample_size:** The number of sample-size of data sub-samples with which each binary tree will be built. The default for this parameter is the length of the dataset. For memory management, we will try to use 64 sample sizes as the first attempt.

- **ntrees:** Number of binary trees to build for the model. The default for these parameters is 10.

The Isolation Forest Algorithm will produce an anomaly score within the range of 0 to 1. The interpretation of an anomaly score can be determined here:

- If the anomaly score is close to 1, it can be interpreted that the point is an anomaly.

- If the anomaly score is minimal compared to 0.5, it can be interpreted as a regular data point.

- If the anomaly score produced, all of them, is 0.5, then it can be confirmed that there is no anomaly for the sampling dataset.


```{r}
fraud_score <- predict(fraud_isotree, newdata = fraud_train %>% select(-isFraud))
fraud_train$score <- fraud_score
```

we will divide the anomaly score with the a spesific threshold. If the anomaly score is more than 80% quantile for the rest of data, we will clasify the point as an anomaly. This threshold can be tolerated with respective subject business matter.


```{r}
quantile(fraud_train$score, probs = c(0, 0.8))

```

```{r}
fraud_train <- fraud_train %>% 
  mutate(fraud_detection = as.factor(ifelse(fraud_score >= 0.4821847 , 1, 0)))
```


## Visualizing The Isolation Forest Score

A contour plot can visualize the score produced by the Isolation Forest algorithm. The visualization will give us more insight into the polarization of the anomaly score. Since the visualization is limited to fewer dimensions, we will compress the data into two-dimension using PCA; then, we will take two dimensions produced by the PCA and visualize it in a contour plot provided by the lattice library.
```{r}
library(lattice)
library(FactoMineR)
library(factoextra)

fraud_pca_train <- PCA(fraud_train %>% select(step, amount, oldbalanceOrg, newbalanceOrig, oldbalanceDest, newbalanceDest, isFlaggedFraud), scale.unit = T, ncp = 7, graph = F)
```

```{r}
fviz_eig(fraud_pca_train, ncp = 9, addlabels = T, main = "Variance explained by each dimensions")
```


```{r}
pca_grid <- as.data.frame(fraud_pca_train$ind)
```


```{r}
pca_grid <- pca_grid %>%
  select(coord.Dim.1, coord.Dim.2)
```


```{r}
d1_seq <- seq(min(pca_grid$coord.Dim.1), max(pca_grid$coord.Dim.1), length.out = 100)
d2_seq <- seq(min(pca_grid$coord.Dim.2), max(pca_grid$coord.Dim.2), length.out = 100)

fraud_train_grid <- expand.grid(d1 = d1_seq, d2 = d2_seq)
head(fraud_train_grid)
```


```{r}
library(isotree)

pca_isotree <- isolation.forest(fraud_train_grid, sample_size = 64)

fraud_train_grid$score <- predict(pca_isotree, fraud_train_grid)
```


```{r}
library(lattice)
# Define a custom color palette

# Create the contour plot with the custom color palette
contourplot(score ~ d1 + d2, fraud_train_grid, region = TRUE)
```


middle white space represents the points polarizing as the normal data transactions; here, we called genuine transactions. In contrast, the darker blue area is the region that far from the normal instances. The Isolation Forest will take the points that fall far from the normal point polarization as an anomaly.

